{"ast":null,"code":"import _asyncToGenerator from \"C:\\\\Users\\\\sdeshpande\\\\Documents\\\\FluidExamples\\\\angular-demo\\\\node_modules\\\\@babel\\\\runtime\\\\helpers\\\\esm\\\\asyncToGenerator.js\";\n\n/*!\n * Copyright (c) Microsoft Corporation and contributors. All rights reserved.\n * Licensed under the MIT License.\n */\nimport { assert, Deferred, performance } from \"@fluidframework/common-utils\";\nimport { PerformanceEvent } from \"@fluidframework/telemetry-utils\";\nimport { getRetryDelayFromError, canRetryOnError, createGenericNetworkError } from \"./network\";\nimport { waitForConnectedState, logNetworkFailure } from \"./networkUtils\"; // For now, this package is versioned and released in unison with the specific drivers\n\nimport { pkgVersion as driverVersion } from \"./packageVersion\";\nconst MaxFetchDelayInMs = 10000;\nconst MissingFetchDelayInMs = 100;\n/**\n * Helper class to organize parallel fetching of data\n * It can be used to concurrently do many requests, while consuming\n * data in the right order. Take a look at UT for examples.\n * @param concurrency - level of concurrency\n * @param from - starting point of fetching data (inclusive)\n * @param to  - ending point of fetching data. exclusive, or undefined if unknown\n * @param payloadSize - batch size\n * @param logger - logger to use\n * @param requestCallback - callback to request batches\n * @returns - Queue that can be used to retrieve data\n */\n\nexport class ParallelRequests {\n  constructor(from, to, payloadSize, logger, requestCallback, responseCallback) {\n    this.to = to;\n    this.payloadSize = payloadSize;\n    this.logger = logger;\n    this.requestCallback = requestCallback;\n    this.responseCallback = responseCallback;\n    this.results = new Map();\n    this.workingState = \"working\";\n    this.requestsInFlight = 0;\n    this.endEvent = new Deferred();\n    this.requests = 0;\n    this.latestRequested = from;\n    this.nextToDeliver = from;\n    this.knewTo = to !== undefined;\n  }\n\n  get working() {\n    return this.workingState === \"working\";\n  }\n\n  get canceled() {\n    return this.workingState === \"canceled\";\n  }\n\n  cancel() {\n    if (this.working) {\n      this.workingState = \"canceled\";\n      this.endEvent.resolve();\n    }\n  }\n\n  run(concurrency) {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      assert(concurrency > 0, 0x102\n      /* \"invalid level of concurrency\" */\n      );\n      assert(_this.working, 0x103\n      /* \"trying to parallel run while not working\" */\n      );\n      let c = concurrency;\n\n      while (c > 0) {\n        c--;\n\n        _this.addRequest();\n      }\n\n      _this.dispatch(); // will recalculate and trigger this.endEvent if needed\n\n\n      return _this.endEvent.promise;\n    })();\n  }\n\n  done() {\n    // We should satisfy request fully.\n    assert(this.to !== undefined, 0x104\n    /* \"undefined end point for parallel fetch\" */\n    );\n    assert(this.nextToDeliver >= this.to, 0x105\n    /* \"unexpected end point for parallel fetch\" */\n    );\n\n    if (this.working) {\n      this.workingState = \"done\";\n      this.endEvent.resolve();\n    }\n  }\n\n  fail(error) {\n    if (this.working) {\n      this.workingState = \"done\";\n      this.endEvent.reject(error);\n    }\n  }\n\n  dispatch() {\n    while (this.working) {\n      const value = this.results.get(this.nextToDeliver);\n\n      if (value === undefined) {\n        break;\n      }\n\n      this.results.delete(this.nextToDeliver);\n      assert(value.length <= this.payloadSize, 0x1d9\n      /* \"addRequestCore() should break into smaller chunks\" */\n      );\n      this.nextToDeliver += value.length;\n      this.responseCallback(value);\n    } // Account for cancellation - state might be not in consistent state on cancelling operation\n\n\n    if (this.working) {\n      if (this.requestsInFlight === 0) {\n        // we should have dispatched everything, no matter whether we knew about the end or not.\n        // see comment in addRequestCore() around throwing away chunk if it's above this.to\n        assert(this.results.size === 0, 0x107\n        /* \"ending dispatch with remaining results to be sent\" */\n        );\n        this.done();\n      } else if (this.to !== undefined && this.nextToDeliver >= this.to) {\n        // Learned about the end and dispatched all the ops up to it.\n        // Ignore all the in-flight requests above boundary - unblock caller sooner.\n        assert(!this.knewTo, 0x108\n        /* \"ending results dispatch but knew in advance about more requests\" */\n        );\n        this.done();\n      }\n    }\n  }\n\n  getNextChunk() {\n    if (!this.working) {\n      return undefined;\n    }\n\n    const from = this.latestRequested;\n\n    if (this.to !== undefined) {\n      if (this.to <= from) {\n        return undefined;\n      }\n    } // this.latestRequested\n    // inclusive on the right side! Exclusive on the left.\n\n\n    this.latestRequested += this.payloadSize;\n\n    if (this.to !== undefined) {\n      this.latestRequested = Math.min(this.to, this.latestRequested);\n    }\n\n    assert(from < this.latestRequested, 0x109\n    /* \"unexpected next chunk position\" */\n    );\n    return {\n      from,\n      to: this.latestRequested\n    };\n  }\n\n  addRequest() {\n    const chunk = this.getNextChunk();\n\n    if (chunk === undefined) {\n      return;\n    }\n\n    this.addRequestCore(chunk.from, chunk.to).catch(this.fail.bind(this));\n  }\n\n  addRequestCore(fromArg, toArg) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      assert(_this2.working, 0x10a\n      /* \"cannot add parallel request while not working\" */\n      );\n      let from = fromArg;\n      let to = toArg; // to & from are exclusive\n\n      _this2.requestsInFlight++;\n\n      while (_this2.working) {\n        const requestedLength = to - from;\n        assert(requestedLength > 0, 0x10b\n        /* \"invalid parallel request range\" */\n        ); // We should not be wasting time asking for something useless.\n\n        if (_this2.to !== undefined) {\n          assert(from < _this2.to, 0x10c\n          /* \"invalid parallel request start point\" */\n          );\n          assert(to <= _this2.to, 0x10d\n          /* \"invalid parallel request end point\" */\n          );\n        }\n\n        _this2.requests++;\n\n        const promise = _this2.requestCallback(_this2.requests, from, to, _this2.to !== undefined, {}); // dispatch any prior received data\n\n\n        _this2.dispatch();\n\n        const {\n          payload,\n          cancel,\n          partial\n        } = yield promise;\n\n        if (cancel) {\n          _this2.cancel();\n        }\n\n        if (_this2.to !== undefined && from >= _this2.to) {\n          // while we were waiting for response, we learned on what is the boundary\n          // We can get here (with actual result!) if situation changed while this request was in\n          // flight, i.e. the end was extended over what we learn in some other request\n          // While it's useful not to throw this result, this is very corner cases and makes logic\n          // (including consistency checks) much harder to write correctly.\n          // So for now, we are throwing this result out the window.\n          assert(!_this2.knewTo, 0x10e\n          /* \"should not throw result if we knew about boundary in advance\" */\n          ); // Learn how often it happens and if it's too wasteful to throw these chunks.\n          // If it pops into our view a lot, we would need to reconsider how we approach it.\n          // Note that this is not visible to user other than potentially not hitting 100% of\n          // what we can in perf domain.\n\n          if (payload.length !== 0) {\n            _this2.logger.sendErrorEvent({\n              eventName: \"ParallelRequests_GotExtra\",\n              from,\n              to,\n              end: _this2.to,\n              length: payload.length\n            });\n          }\n\n          break;\n        }\n\n        if (_this2.working) {\n          const fromOrig = from;\n          const length = payload.length;\n          let fullChunk = requestedLength <= length; // we can possible get more than we asked.\n\n          if (length !== 0) {\n            // We can get more than we asked for!\n            // This can screw up logic in dispatch!\n            // So push only batch size, and keep the rest for later - if conditions are favorable, we\n            // will be able to use it. If not (parallel request overlapping these ops), it's easier to\n            // discard them and wait for another (overlapping) request to come in later.\n            if (requestedLength < length) {\n              // This is error in a sense that it's not expected and likely points bug in other layer.\n              // This layer copes with this situation just fine.\n              _this2.logger.sendTelemetryEvent({\n                eventName: \"ParallelRequests_Over\",\n                from,\n                to,\n                length\n              });\n            }\n\n            const data = payload.splice(0, requestedLength);\n\n            _this2.results.set(from, data);\n\n            from += data.length;\n          } else {\n            // 1. empty (partial) chunks should not be returned by various caching / adapter layers -\n            //    they should fall back to next layer. This might be important invariant to hold to ensure\n            //    that we are less likely have bugs where such layer would keep returning empty partial\n            //    result on each call.\n            // 2. Current invariant is that callback does retries until it gets something,\n            //    with the goal of failing if zero data is retrieved in given amount of time.\n            //    This is very specific property of storage / ops, so this logic is not here, but given only\n            //    one user of this class, we assert that to catch issues earlier.\n            // These invariant can be relaxed if needed.\n            assert(!partial, 0x10f\n            /* \"empty/partial chunks should not be returned by caching\" */\n            );\n            assert(!_this2.knewTo, 0x110\n            /* \"callback should retry until valid fetch before it learns new boundary\" */\n            );\n          }\n\n          if (!partial && !fullChunk) {\n            if (!_this2.knewTo) {\n              if (_this2.to === undefined || _this2.to > from) {\n                // The END\n                _this2.to = from;\n              }\n\n              break;\n            } // We know that there are more items to be retrieved\n            // Can we get partial chunk? Ideally storage indicates that's not a full chunk\n            // Note that it's possible that not all ops hit storage yet.\n            // We will come back to request more, and if we can't get any more ops soon, it's\n            // catastrophic failure (see comment above on responsibility of callback to return something)\n            // This layer will just keep trying until it gets full set.\n\n\n            _this2.logger.sendPerformanceEvent({\n              eventName: \"ParallelRequests_Partial\",\n              from: fromOrig,\n              to,\n              length\n            });\n          }\n\n          if (to === _this2.latestRequested) {\n            // we can go after full chunk at the end if we received partial chunk, or more than asked\n            // Also if we got more than we asked to, we can actually use those ops!\n            if (payload.length !== 0) {\n              _this2.results.set(from, payload);\n\n              from += payload.length;\n            }\n\n            _this2.latestRequested = from;\n            fullChunk = true;\n          }\n\n          if (fullChunk) {\n            const chunk = _this2.getNextChunk();\n\n            if (chunk === undefined) {\n              break;\n            }\n\n            from = chunk.from;\n            to = chunk.to;\n          }\n        }\n      }\n\n      _this2.requestsInFlight--;\n\n      _this2.dispatch();\n    })();\n  }\n\n}\n/**\n * Helper queue class to allow async push / pull\n * It's essentially a pipe allowing multiple writers, and single reader\n */\n\nexport class Queue {\n  constructor() {\n    this.queue = [];\n    this.done = false;\n  }\n\n  pushValue(value) {\n    this.pushCore(Promise.resolve({\n      done: false,\n      value\n    }));\n  }\n\n  pushError(error) {\n    this.pushCore(Promise.reject(error));\n    this.done = true;\n  }\n\n  pushDone() {\n    this.pushCore(Promise.resolve({\n      done: true\n    }));\n    this.done = true;\n  }\n\n  pushCore(value) {\n    assert(!this.done, 0x112\n    /* \"cannot push onto queue if done\" */\n    );\n\n    if (this.deferred) {\n      assert(this.queue.length === 0, 0x113\n      /* \"deferred queue should be empty\" */\n      );\n      this.deferred.resolve(value);\n      this.deferred = undefined;\n    } else {\n      this.queue.push(value);\n    }\n  }\n\n  read() {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      assert(_this3.deferred === undefined, 0x114\n      /* \"cannot pop if deferred\" */\n      );\n\n      const value = _this3.queue.shift();\n\n      if (value !== undefined) {\n        return value;\n      }\n\n      assert(!_this3.done, 0x115\n      /* \"queue should not be done during pop\" */\n      );\n      _this3.deferred = new Deferred();\n      return _this3.deferred.promise;\n    })();\n  }\n\n}\n/**\n * Retrieve single batch of ops\n * @param request - request index\n * @param from - inclusive boundary\n * @param to - exclusive boundary\n * @param telemetryEvent - telemetry event used to track consecutive batch of requests\n * @param strongTo - tells if ops in range from...to have to be there and have to be retrieved.\n * If false, returning less ops would mean we reached end of file.\n * @param logger - logger object to use to log progress & errors\n * @param signal - cancelation signal\n * @param scenarioName - reason for fetching ops\n * @returns - an object with resulting ops and cancellation / partial result flags\n */\n\nfunction getSingleOpBatch(_x, _x2, _x3, _x4, _x5, _x6) {\n  return _getSingleOpBatch.apply(this, arguments);\n}\n/**\n * Request ops from storage\n * @param get - Getter callback to get individual batches\n * @param concurrency - Number of concurrent requests to make\n * @param fromTotal - starting sequence number to fetch (inclusive)\n * @param toTotal - max (exclusive) sequence number to fetch\n * @param payloadSize - Payload size\n * @param logger - Logger to log progress and errors\n * @param signal - Cancelation signal\n * @param scenarioName - Reason for fetching ops\n * @returns - Messages fetched\n */\n\n\nfunction _getSingleOpBatch() {\n  _getSingleOpBatch = _asyncToGenerator(function* (get, props, strongTo, logger, signal, scenarioName) {\n    let lastSuccessTime;\n    let retry = 0;\n    const deltas = [];\n    const nothing = {\n      partial: false,\n      cancel: true,\n      payload: []\n    };\n\n    while ((signal === null || signal === void 0 ? void 0 : signal.aborted) !== true) {\n      retry++;\n      let delay = Math.min(MaxFetchDelayInMs, MissingFetchDelayInMs * Math.pow(2, retry));\n      const startTime = performance.now();\n\n      try {\n        // Issue async request for deltas - limit the number fetched to MaxBatchDeltas\n        const deltasP = get(Object.assign(Object.assign({}, props), {\n          retry\n        }));\n        const {\n          messages,\n          partialResult\n        } = yield deltasP;\n        deltas.push(...messages);\n        const deltasRetrievedLast = messages.length;\n\n        if (deltasRetrievedLast !== 0 || !strongTo) {\n          return {\n            payload: deltas,\n            cancel: false,\n            partial: partialResult\n          };\n        } // Storage does not have ops we need.\n        // Attempt to fetch more deltas. If we didn't receive any in the previous call we up our retry\n        // count since something prevented us from seeing those deltas\n\n\n        if (lastSuccessTime === undefined) {\n          lastSuccessTime = performance.now();\n        } else if (performance.now() - lastSuccessTime > 30000) {\n          // If we are connected and receiving proper responses from server, but can't get any ops back,\n          // then give up after some time. This likely indicates the issue with ordering service not flushing\n          // ops to storage quick enough, and possibly waiting for summaries, while summarizer can't get\n          // current as it can't get ops.\n          throw createGenericNetworkError( // pre-0.58 error message: failedToRetrieveOpsFromStorage:TooManyRetries\n          \"Failed to retrieve ops from storage (Too Many Retries)\", {\n            canRetry: false\n          }, Object.assign({\n            retry,\n            driverVersion\n          }, props));\n        }\n      } catch (error) {\n        const canRetry = canRetryOnError(error);\n        lastSuccessTime = undefined;\n        const retryAfter = getRetryDelayFromError(error); // This will log to error table only if the error is non-retryable\n\n        logNetworkFailure(logger, Object.assign(Object.assign({\n          eventName: \"GetDeltas_Error\"\n        }, props), {\n          retry,\n          duration: performance.now() - startTime,\n          retryAfter,\n          reason: scenarioName\n        }), error);\n\n        if (!canRetry) {\n          // It's game over scenario.\n          throw error;\n        }\n\n        if (retryAfter !== undefined && retryAfter >= 0) {\n          delay = retryAfter;\n        }\n      }\n\n      yield waitForConnectedState(delay);\n    }\n\n    return nothing;\n  });\n  return _getSingleOpBatch.apply(this, arguments);\n}\n\nexport function requestOps(get, concurrency, fromTotal, toTotal, payloadSize, logger, signal, scenarioName) {\n  let requests = 0;\n  let lastFetch;\n  let length = 0;\n  const queue = new Queue();\n  const propsTotal = {\n    fromTotal,\n    toTotal\n  };\n  const telemetryEvent = PerformanceEvent.start(logger, Object.assign(Object.assign({\n    eventName: \"GetDeltas\"\n  }, propsTotal), {\n    reason: scenarioName\n  }));\n  const manager = new ParallelRequests(fromTotal, toTotal, payloadSize, logger, /*#__PURE__*/function () {\n    var _ref = _asyncToGenerator(function* (request, from, to, strongTo, propsPerRequest) {\n      requests++;\n      return getSingleOpBatch( /*#__PURE__*/function () {\n        var _ref2 = _asyncToGenerator(function* (propsAll) {\n          return get(from, to, propsAll);\n        });\n\n        return function (_x12) {\n          return _ref2.apply(this, arguments);\n        };\n      }(), Object.assign(Object.assign({\n        request,\n        from,\n        to\n      }, propsTotal), propsPerRequest), strongTo, logger, signal, scenarioName);\n    });\n\n    return function (_x7, _x8, _x9, _x10, _x11) {\n      return _ref.apply(this, arguments);\n    };\n  }(), deltas => {\n    // Assert continuing and right start.\n    if (lastFetch === undefined) {\n      assert(deltas[0].sequenceNumber === fromTotal, 0x26d\n      /* \"wrong start\" */\n      );\n    } else {\n      assert(deltas[0].sequenceNumber === lastFetch + 1, 0x26e\n      /* \"wrong start\" */\n      );\n    }\n\n    lastFetch = deltas[deltas.length - 1].sequenceNumber;\n    assert(lastFetch - deltas[0].sequenceNumber + 1 === deltas.length, 0x26f\n    /* \"continuous and no duplicates\" */\n    );\n    length += deltas.length;\n    queue.pushValue(deltas);\n  }); // Implement faster cancellation. getSingleOpBatch() checks signal, but only in between\n  // waits (up to 10 seconds) and fetches (can take infinite amount of time).\n  // While every such case should be improved and take into account signal (and thus cancel immediately),\n  // it is beneficial to have catch-all\n\n  const listener = event => {\n    manager.cancel();\n  };\n\n  if (signal !== undefined) {\n    signal.addEventListener(\"abort\", listener);\n  }\n\n  manager.run(concurrency).finally(() => {\n    if (signal !== undefined) {\n      signal.removeEventListener(\"abort\", listener);\n    }\n  }).then(() => {\n    const props = {\n      lastFetch,\n      length,\n      requests\n    };\n\n    if (manager.canceled) {\n      telemetryEvent.cancel(Object.assign(Object.assign({}, props), {\n        error: \"ops request cancelled by client\"\n      }));\n    } else {\n      assert(toTotal === undefined || lastFetch !== undefined && lastFetch >= toTotal - 1, 0x270\n      /* \"All requested ops fetched\" */\n      );\n      telemetryEvent.end(props);\n    }\n\n    queue.pushDone();\n  }).catch(error => {\n    telemetryEvent.cancel({\n      lastFetch,\n      length,\n      requests\n    }, error);\n    queue.pushError(error);\n  });\n  return queue;\n}\nexport const emptyMessageStream = {\n  read: function () {\n    var _ref3 = _asyncToGenerator(function* () {\n      return {\n        done: true\n      };\n    });\n\n    return function read() {\n      return _ref3.apply(this, arguments);\n    };\n  }()\n};\nexport function streamFromMessages(messagesArg) {\n  let messages = messagesArg;\n  return {\n    read: function () {\n      var _ref4 = _asyncToGenerator(function* () {\n        if (messages === undefined) {\n          return {\n            done: true\n          };\n        }\n\n        const value = yield messages;\n        messages = undefined;\n        return value.length === 0 ? {\n          done: true\n        } : {\n          done: false,\n          value\n        };\n      });\n\n      return function read() {\n        return _ref4.apply(this, arguments);\n      };\n    }()\n  };\n}\nexport function streamObserver(stream, handler) {\n  return {\n    read: function () {\n      var _ref5 = _asyncToGenerator(function* () {\n        const value = yield stream.read();\n        handler(value);\n        return value;\n      });\n\n      return function read() {\n        return _ref5.apply(this, arguments);\n      };\n    }()\n  };\n} //# sourceMappingURL=parallelRequests.js.map","map":null,"metadata":{},"sourceType":"module"}