{"ast":null,"code":"import _asyncToGenerator from \"C:\\\\Users\\\\sdeshpande\\\\Documents\\\\FluidExamples\\\\angular-demo\\\\node_modules\\\\@babel\\\\runtime\\\\helpers\\\\esm\\\\asyncToGenerator.js\";\n\n/*!\n * Copyright (c) Microsoft Corporation and contributors. All rights reserved.\n * Licensed under the MIT License.\n */\nimport { assert, bufferToString } from \"@fluidframework/common-utils\";\nimport { ChildLogger } from \"@fluidframework/telemetry-utils\";\nimport { SummaryTreeBuilder } from \"@fluidframework/runtime-utils\";\nimport { UnassignedSequenceNumber } from \"./constants\";\nimport { matchProperties } from \"./properties\";\nimport { toLatestVersion, serializeAsMaxSupportedVersion } from \"./snapshotChunks\";\nimport { SnapshotLegacy } from \"./snapshotlegacy\";\nexport let SnapshotV1 = /*#__PURE__*/(() => {\n  class SnapshotV1 {\n    constructor(mergeTree, logger, getLongClientId, filename, onCompletion) {\n      var _a, _b;\n\n      this.mergeTree = mergeTree;\n      this.getLongClientId = getLongClientId;\n      this.filename = filename;\n      this.onCompletion = onCompletion;\n      this.logger = ChildLogger.create(logger, \"Snapshot\");\n      this.chunkSize = (_b = (_a = mergeTree === null || mergeTree === void 0 ? void 0 : mergeTree.options) === null || _a === void 0 ? void 0 : _a.mergeTreeSnapshotChunkSize) !== null && _b !== void 0 ? _b : SnapshotV1.chunkSize;\n      const {\n        currentSeq,\n        minSeq\n      } = mergeTree.getCollabWindow();\n      this.header = {\n        minSequenceNumber: minSeq,\n        sequenceNumber: currentSeq,\n        orderedChunkMetadata: [],\n        totalLength: 0,\n        totalSegmentCount: 0\n      };\n      this.segments = [];\n      this.segmentLengths = [];\n    }\n\n    getSeqLengthSegs(allSegments, allLengths, approxSequenceLength, startIndex = 0) {\n      const segments = [];\n      let length = 0;\n      let segmentCount = 0;\n\n      while (length < approxSequenceLength && startIndex + segmentCount < allSegments.length) {\n        const pseg = allSegments[startIndex + segmentCount];\n        segments.push(pseg);\n        length += allLengths[startIndex + segmentCount];\n        segmentCount++;\n      }\n\n      return {\n        version: \"1\",\n        segmentCount,\n        length,\n        segments,\n        startIndex,\n        headerMetadata: undefined\n      };\n    }\n    /**\n     * Emits the snapshot to an ISummarizeResult. If provided the optional IFluidSerializer will be used when\n     * serializing the summary data rather than JSON.stringify.\n     */\n\n\n    emit(serializer, bind) {\n      const chunks = [];\n      this.header.totalSegmentCount = 0;\n      this.header.totalLength = 0;\n\n      do {\n        const chunk = this.getSeqLengthSegs(this.segments, this.segmentLengths, this.chunkSize, this.header.totalSegmentCount);\n        chunks.push(chunk);\n        this.header.totalSegmentCount += chunk.segmentCount;\n        this.header.totalLength += chunk.length;\n      } while (this.header.totalSegmentCount < this.segments.length); // The do while loop should have added at least one chunk\n      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n\n\n      const headerChunk = chunks.shift();\n      headerChunk.headerMetadata = this.header;\n      headerChunk.headerMetadata.orderedChunkMetadata = [{\n        id: SnapshotLegacy.header\n      }];\n      const blobs = [];\n      chunks.forEach((chunk, index) => {\n        const id = `${SnapshotLegacy.body}_${index}`;\n        this.header.orderedChunkMetadata.push({\n          id\n        });\n        blobs.push([id, serializeAsMaxSupportedVersion(id, chunk, this.logger, this.mergeTree.options, serializer, bind)]);\n      });\n      const builder = new SummaryTreeBuilder();\n      builder.addBlob(SnapshotLegacy.header, serializeAsMaxSupportedVersion(SnapshotLegacy.header, headerChunk, this.logger, this.mergeTree.options, serializer, bind));\n      blobs.forEach(value => {\n        builder.addBlob(value[0], value[1]);\n      });\n      return builder.getSummaryTree();\n    }\n\n    extractSync() {\n      const mergeTree = this.mergeTree;\n      const minSeq = this.header.minSequenceNumber; // Helper to add the given `MergeTreeChunkV0SegmentSpec` to the snapshot.\n\n      const pushSegRaw = (json, length) => {\n        this.segments.push(json);\n        this.segmentLengths.push(length);\n      }; // Helper to serialize the given `segment` and add it to the snapshot (if a segment is provided).\n\n\n      const pushSeg = segment => {\n        if (segment) {\n          pushSegRaw(segment.toJSONObject(), segment.cachedLength);\n        }\n      };\n\n      let prev;\n\n      const extractSegment = segment => {\n        var _a; // Elide segments that do not need to be included in the snapshot.  A segment may be elided if\n        // either condition is true:\n        //   a) The segment has not yet been ACKed.  We do not need to snapshot unACKed segments because\n        //      there is a pending insert op that will deliver the segment on reconnection.\n        //   b) The segment was removed at or below the MSN.  Pending ops can no longer reference this\n        //      segment, and therefore we can discard it.\n        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n\n\n        if (segment.seq === UnassignedSequenceNumber || segment.removedSeq <= minSeq) {\n          return true;\n        } // Next determine if the snapshot needs to preserve information required for merging the segment\n        // (seq, client, etc.)  This information is only needed if the segment is above the MSN (and doesn't\n        // have a pending remove.)\n        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n\n\n        if (segment.seq <= minSeq // Segment is below the MSN, and...\n        && (segment.removedSeq === undefined // .. Segment has not been removed, or...\n        || segment.removedSeq === UnassignedSequenceNumber) // .. Removal op to be delivered on reconnect\n        ) {\n          // This segment is below the MSN, which means that future ops will not reference it.  Attempt to\n          // coalesce the new segment with the previous (if any).\n          if (!prev) {\n            // We do not have a previous candidate for coalescing.  Make the current segment the new candidate.\n            prev = segment;\n          } else if (prev.canAppend(segment) && matchProperties(prev.properties, segment.properties)) {\n            // We have a compatible pair.  Replace `prev` with the coalesced segment.  Clone to avoid\n            // modifying the segment instances currently in the MergeTree.\n            prev = prev.clone();\n            prev.append(segment.clone());\n          } else {\n            // The segment pair could not be coalesced.  Record the `prev` segment in the snapshot\n            // and make the current segment the new candidate for coalescing.\n            pushSeg(prev);\n            prev = segment;\n          }\n        } else {\n          // This segment needs to preserve it's metadata as it may be referenced by future ops.  It's ineligible\n          // for coalescing, so emit the 'prev' segment now (if any).\n          pushSeg(prev);\n          prev = undefined;\n          const raw = {\n            json: segment.toJSONObject()\n          }; // If the segment insertion is above the MSN, record the insertion merge info.\n          // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n\n          if (segment.seq > minSeq) {\n            raw.seq = segment.seq;\n            raw.client = this.getLongClientId(segment.clientId);\n          } // We have already dispensed with removed segments below the MSN and removed segments with unassigned\n          // sequence numbers.  Any remaining removal info should be preserved.\n\n\n          if (segment.removedSeq !== undefined) {\n            assert(segment.removedSeq !== UnassignedSequenceNumber && segment.removedSeq > minSeq, 0x065\n            /* \"On removal info preservation, segment has invalid removed sequence number!\" */\n            );\n            raw.removedSeq = segment.removedSeq; // back compat for when we split overlap and removed client\n\n            raw.removedClient = segment.removedClientIds !== undefined ? this.getLongClientId(segment.removedClientIds[0]) : undefined;\n            raw.removedClientIds = (_a = segment.removedClientIds) === null || _a === void 0 ? void 0 : _a.map(id => this.getLongClientId(id));\n          } // Sanity check that we are preserving either the seq < minSeq or a removed segment's info.\n\n\n          assert(raw.seq !== undefined && raw.client !== undefined || raw.removedSeq !== undefined && raw.removedClient !== undefined, 0x066\n          /* \"Corrupted preservation of segment metadata!\" */\n          ); // Record the segment with it's required metadata.\n\n          pushSegRaw(raw, segment.cachedLength);\n        }\n\n        return true;\n      };\n\n      mergeTree.walkAllSegments(mergeTree.root, extractSegment, this); // If the last segment in the walk was coalescable, push it now.\n\n      pushSeg(prev);\n      return this.segments;\n    }\n\n    static loadChunk(storage, path, logger, options, serializer) {\n      return _asyncToGenerator(function* () {\n        const blob = yield storage.readBlob(path);\n        const chunkAsString = bufferToString(blob, \"utf8\");\n        return SnapshotV1.processChunk(path, chunkAsString, logger, options, serializer);\n      })();\n    }\n\n    static processChunk(path, chunk, logger, options, serializer) {\n      const chunkObj = serializer ? serializer.parse(chunk) : JSON.parse(chunk);\n      return toLatestVersion(path, chunkObj, logger, options);\n    }\n\n  }\n\n  // Split snapshot into two entries - headers (small) and body (overflow) for faster loading initial content\n  // Please note that this number has no direct relationship to anything other than size of raw text (characters).\n  // As we produce json for the blob (and then send over the wire compressed), this number\n  // is really hard to correlate with any actual metric that matters (like bytes over the wire).\n  // For test with small number of chunks it would be closer to blob size,\n  // for very chunky text, blob size can easily be 4x-8x of that number.\n  //# sourceMappingURL=snapshotV1.js.map\n  SnapshotV1.chunkSize = 10000;\n  return SnapshotV1;\n})();","map":null,"metadata":{},"sourceType":"module"}